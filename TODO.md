- I removed query mode because I don't like it really and it wastes API calls, but also there's the chance that the listing title wouldn't contain the keyword but the description would, so maybe I should re-add it with that in mind? Or somehow get the description in the current mode? (That would probably require calls for each listing though which would be super expensive since the endpoint for getting listing details shares a rate limit with the endpoint for searching listings, however I'm pretty sure it supports batching: `https://api.ebay.com/buy/browse/v1/item?item_ids=<id1>,<id2>,...` so it would basically just be one extra call, idk i'll consider it)
    - example of this: ssds are pretty hard to match since even with an advanced regex query searching for stuff like 500gb tends to match laptops / prebuilts too, if i re-implemented support for ebay's search feature then i could just query "500gb nvme ssd" or something and ebay's much smarter searching system knows (for the most part) what listings are actually SSDs and what listings are not, and then i could do the same price and deal matching stuff on those results
    - or i could just re-add it as it was before and only use it for "hot items" and/or items that are harder to filter with regex - ssds are a big one due to prebuilts and laptops also containing "1TB NVMe SSD" or simmilar in the title
- add target price as a key in the keyword config objects instead of a comment, that way it can be used
- maybe combine the self roles thing with the ping configs / keyword configs instead of separate since they share a role and name and stuff
- add priority listings that poll more often
- add warning for low feedback / low rating sellers
- this is probably possible by copying channel id across multiple areas of the configuration file, but make sure it's possible to have multiple ping configs going to the same channel
- as long as this isn't against ebay terms of service and as long as rate limits are per api key and not per ip, create a second developer account and set up a dual API system to double the amount of daily calls available
- note: lots of the above suggestions to myself are meant to save API calls, this is so I can do more calls and more often searches across more categories improving the scraper overall
- write docstrings (some but not all have been written)